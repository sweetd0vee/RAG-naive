import getpass
import logging
import os
import shutil
from typing import Any, Dict, List, Optional

import chromadb
from chromadb.config import Settings
from dotenv import load_dotenv
from langchain.chains import RetrievalQA
from langchain.llms.huggingface_pipeline import HuggingFacePipeline
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama  # –∏–ª–∏ –¥—Ä—É–≥–∞—è LLM
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import BasePromptTemplate
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# Load environment variables from .env file
load_dotenv()

hf_token = os.environ.get("HUGGINGFACEHUB_API_TOKEN")


class SimpleRAG:
    def __init__(self, pdf_path: str, persist_directory: str = "./chroma_db"):
        self.pdf_path = pdf_path
        self.persist_directory = persist_directory
        self.vectorstore = None
        self.qa_chain = None
        self.embeddings = None

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

    def load_and_process_document(self, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Document]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""

        if not os.path.exists(self.pdf_path):
            raise FileNotFoundError(f"PDF —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.pdf_path}")

        self.logger.info("–ó–∞–≥—Ä—É–∂–∞—é –¥–æ–∫—É–º–µ–Ω—Ç...")

        # –ó–∞–≥—Ä—É–∑–∫–∞ PDF
        loader = PyPDFLoader(self.pdf_path)
        documents = loader.load()

        if not documents:
            raise ValueError("–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞")

        # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        chunks = text_splitter.split_documents(documents)
        self.logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞")

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤
        valid_chunks = self._validate_chunks(chunks)
        self.logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(valid_chunks)} –≤–∞–ª–∏–¥–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")

        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã
        self._create_vectorstore(valid_chunks)

        return valid_chunks

    def _validate_chunks(self, chunks: List[Document]) -> List[Document]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ —á–∞–Ω–∫–æ–≤"""
        valid_chunks = []

        for i, chunk in enumerate(chunks):
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
            if not chunk.page_content or not chunk.page_content.strip():
                continue

            # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            cleaned_content = self._clean_text(chunk.page_content)
            if len(cleaned_content) < 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
                continue

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            chunk.page_content = cleaned_content
            if 'page' not in chunk.metadata:
                chunk.metadata['page'] = 0
            chunk.metadata['source'] = self.pdf_path
            chunk.metadata['chunk_id'] = i

            valid_chunks.append(chunk)

        return valid_chunks

    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        import re

        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text)
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        text = re.sub(r'[^\w\s.,!?;:()\-]', '', text)
        return text.strip()

    def _create_vectorstore(self, chunks: List[Document]):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä–æ–π –±–∞–∑—ã
        if os.path.exists(self.persist_directory):
            shutil.rmtree(self.persist_directory)

        self.logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã...")

        try:
            # –°–ø–æ—Å–æ–± 1: –°–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π (—Ä–∞–±–æ—Ç–∞–µ—Ç —Å chromadb==0.4.22)
            self.vectorstore = Chroma.from_documents(
                documents=chunks,
                embedding=self.embeddings,
                persist_directory=self.persist_directory
            )
            # –ù–µ –≤—ã–∑—ã–≤–∞–µ–º persist() - –æ–Ω –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
            self.logger.info("–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ (—Å–ø–æ—Å–æ–± 1)")

        except Exception as e:
            self.logger.warning(f"–°–ø–æ—Å–æ–± 1 –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
            self.logger.info("–ü—Ä–æ–±—É—é —Å–ø–æ—Å–æ–± 2...")
            self.vectorstore = self._create_vectorstore_faiss(chunks)

    def _create_vectorstore_alternative(self, chunks: List[Document]):
        """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã"""
        try:
            import chromadb

            client = chromadb.PersistentClient(path=self.persist_directory)

            collection = client.get_or_create_collection(
                name="documents",
                metadata={"hnsw:space": "cosine"}
            )

            documents = []
            metadatas = []
            ids = []

            for i, chunk in enumerate(chunks):
                documents.append(chunk.page_content)
                metadatas.append(chunk.metadata)
                ids.append(f"doc_{i}")

            collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )

            vectorstore = Chroma(
                client=client,
                collection_name="documents",
                embedding_function=self.embeddings,
            )

            vectorstore.persist()
            return vectorstore

        except Exception as e:
            self.logger.error(f"–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± —Ç–∞–∫–∂–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
            raise


    def _create_vectorstore_faiss(self, chunks: List[Document]):
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π —Å–ø–æ—Å–æ–± —Å FAISS"""
        try:
            from langchain_community.vectorstores import FAISS

            vectorstore = FAISS.from_documents(chunks, self.embeddings)
            vectorstore.save_local(self.persist_directory)

            self.logger.info("–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ —Å–æ–∑–¥–∞–Ω–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º FAISS")
            return vectorstore

        except Exception as e:
            self.logger.error(f"FAISS —Ç–∞–∫–∂–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
            raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ")


    def setup_qa_chain(self, model_name: str = "llama2", search_k: int = 3):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ü–µ–ø–æ—á–∫–∏ –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç"""

        if not self.vectorstore:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã!")

        # –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        prompt_template = """–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫, –æ—Ç–≤–µ—á–∞—é—â–∏–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {question}

–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
1. –û—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
2. –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, —Å–∫–∞–∂–∏ "–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å"
3. –ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ –ª–∞–∫–æ–Ω–∏—á–Ω—ã–º
4. –ò—Å–ø–æ–ª—å–∑—É–π –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –µ—Å–ª–∏ —É–º–µ—Å—Ç–Ω–æ

–û—Ç–≤–µ—Ç:"""

        PROMPT = BasePromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )

        try:
            llm = Ollama(model=model_name)
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            raise

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
        retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={
                "k": search_k,
                "score_threshold": 0.5  # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏
            }
        )

        self.qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={
                "prompt": PROMPT,
                "verbose": True
            },
            return_source_documents=True,
            verbose=True
        )

        self.logger.info("–¶–µ–ø–æ—á–∫–∞ QA —É—Å–ø–µ—à–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞")

    def ask_question(self, question: str) -> Dict[str, Any]:
        """–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å —Å–∏—Å—Ç–µ–º–µ RAG"""
        if not self.qa_chain:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ QA —Ü–µ–ø–æ—á–∫—É!")

        self.logger.info(f"–í–æ–ø—Ä–æ—Å: {question}")

        try:
            result = self.qa_chain({"query": question})

            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥
            print(f"\n–û—Ç–≤–µ—Ç: {result['result']}")
            print(f"\n–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ ({len(result['source_documents'])}):")

            for i, doc in enumerate(result['source_documents']):
                page = doc.metadata.get('page', 'N/A')
                source = doc.metadata.get('source', 'Unknown')
                preview = doc.page_content[:150] + "..." if len(doc.page_content) > 150 else doc.page_content
                print(f"   {i+1}. –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page} | {source}")
                print(f"      üìÑ {preview}")
                print()

            return {
                "answer": result['result'],
                "source_documents": result['source_documents'],
                "question": question
            }

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞: {e}")
            return {
                "answer": "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.",
                "source_documents": [],
                "question": question,
                "error": str(e)
            }

    def search_similar(self, query: str, k: int = 3) -> List[Document]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LLM"""
        if not self.vectorstore:
            raise ValueError("–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ!")

        return self.vectorstore.similarity_search(query, k=k)

    def get_document_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ"""
        if not self.vectorstore:
            return {"status": "–î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã"}

        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        collection = self.vectorstore._collection
        count = collection.count() if collection else 0

        return {
            "document_path": self.pdf_path,
            "vector_store": self.persist_directory,
            "document_count": count,
            "status": "–ó–∞–≥—Ä—É–∂–µ–Ω–æ"
        }
