import getpass
import logging
import os
import shutil
from typing import Any, Dict, List, Optional

import chromadb
from chromadb.config import Settings
from dotenv import load_dotenv
from langchain.chains import RetrievalQA
from langchain.llms.huggingface_pipeline import HuggingFacePipeline
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import PromptTemplate
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# Load environment variables from .env file
load_dotenv()

hf_token = os.environ.get("HUGGINGFACEHUB_API_TOKEN")


class SimpleRAG:
    def __init__(self, pdf_path: str, persist_directory: str = "./chroma_db"):
        self.pdf_path = pdf_path
        self.persist_directory = persist_directory
        self.vectorstore = None
        self.qa_chain = None
        self.embeddings = None
        self.llm = None

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

    def load_and_process_document(self, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Document]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""

        if not os.path.exists(self.pdf_path):
            raise FileNotFoundError(f"PDF —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.pdf_path}")

        self.logger.info("–ó–∞–≥—Ä—É–∂–∞—é –¥–æ–∫—É–º–µ–Ω—Ç...")

        # –ó–∞–≥—Ä—É–∑–∫–∞ PDF
        loader = PyPDFLoader(self.pdf_path)
        documents = loader.load()

        if not documents:
            raise ValueError("–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞")

        # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", ". ", "! ", "? ", "; ", ": ", " ", ""]
        )
        chunks = text_splitter.split_documents(documents)
        self.logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞")

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤
        valid_chunks = self._validate_chunks(chunks)
        self.logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(valid_chunks)} –≤–∞–ª–∏–¥–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")

        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã
        self._create_vectorstore(valid_chunks)

        return valid_chunks

    def _validate_chunks(self, chunks: List[Document]) -> List[Document]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ —á–∞–Ω–∫–æ–≤"""
        valid_chunks = []

        for i, chunk in enumerate(chunks):
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
            if not chunk.page_content or not chunk.page_content.strip():
                continue

            # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            cleaned_content = self._clean_text(chunk.page_content)
            if len(cleaned_content) < 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
                continue

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            chunk.page_content = cleaned_content
            if 'page' not in chunk.metadata:
                chunk.metadata['page'] = 0
            chunk.metadata['source'] = os.path.basename(self.pdf_path)
            chunk.metadata['chunk_id'] = i

            valid_chunks.append(chunk)

        return valid_chunks

    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        import re

        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text)
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        text = re.sub(r'[^\w\s.,!?;:()\-]', '', text)
        return text.strip()

    def _create_vectorstore(self, chunks: List[Document]):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä–æ–π –±–∞–∑—ã
        if os.path.exists(self.persist_directory):
            shutil.rmtree(self.persist_directory)

        self.logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã...")

        try:
            # –°–ø–æ—Å–æ–± 1: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Chroma
            self.vectorstore = Chroma.from_documents(
                documents=chunks,
                embedding=self.embeddings,
                persist_directory=self.persist_directory
            )
            self.logger.info("–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

        except Exception as e:
            self.logger.warning(f"Chroma –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
            self.logger.info("–ü—Ä–æ–±—É—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å FAISS...")
            self._create_vectorstore_faiss(chunks)

    def _create_vectorstore_faiss(self, chunks: List[Document]):
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π —Å–ø–æ—Å–æ–± —Å FAISS"""
        try:
            from langchain_community.vectorstores import FAISS

            self.vectorstore = FAISS.from_documents(chunks, self.embeddings)
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º FAISS –∏–Ω–¥–µ–∫—Å
            self.vectorstore.save_local(self.persist_directory)
            self.logger.info("–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ —Å–æ–∑–¥–∞–Ω–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º FAISS")

        except Exception as e:
            self.logger.error(f"FAISS —Ç–∞–∫–∂–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
            raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ")

    def setup_qa_chain(self, model_name: str = "llama2", search_k: int = 3,
                       temperature: float = 0.1, max_tokens: int = 512):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ü–µ–ø–æ—á–∫–∏ –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç"""

        if not self.vectorstore:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã!")

        # –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        prompt_template = """–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫, –æ—Ç–≤–µ—á–∞—é—â–∏–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {question}

–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
1. –û—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
2. –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, —Å–∫–∞–∂–∏ "–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å"
3. –ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ –ª–∞–∫–æ–Ω–∏—á–Ω—ã–º
4. –ò—Å–ø–æ–ª—å–∑—É–π –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –µ—Å–ª–∏ —É–º–µ—Å—Ç–Ω–æ
5. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ

–û—Ç–≤–µ—Ç:"""

        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )

        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            self.llm = Ollama(
                model=model_name,
                temperature=temperature,
                num_predict=max_tokens
            )
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            raise

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
        retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={
                "k": search_k,
                "score_threshold": 0.3  # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–π –ø–æ—Ä–æ–≥
            }
        )

        # –°–æ–∑–¥–∞–Ω–∏–µ QA —Ü–µ–ø–∏
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={
                "prompt": PROMPT,
            },
            return_source_documents=True,
            verbose=False  # –û—Ç–∫–ª—é—á–∞–µ–º –ø–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞
        )

        self.logger.info(f"–¶–µ–ø–æ—á–∫–∞ QA —É—Å–ø–µ—à–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ —Å –º–æ–¥–µ–ª—å—é {model_name}")

    def ask_question(self, question: str) -> Dict[str, Any]:
        """–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å —Å–∏—Å—Ç–µ–º–µ RAG"""
        if not self.qa_chain:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ QA —Ü–µ–ø–æ—á–∫—É!")

        self.logger.info(f"–í–æ–ø—Ä–æ—Å: {question}")

        try:
            result = self.qa_chain({"query": question})

            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥
            print(f"\nü§ñ –û—Ç–≤–µ—Ç: {result['result']}")

            if result['source_documents']:
                print(f"\nüìö –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ ({len(result['source_documents'])}:")
                for i, doc in enumerate(result['source_documents']):
                    page = doc.metadata.get('page', 'N/A')
                source = doc.metadata.get('source', 'Unknown')
                preview = doc.page_content[:150] + "..." if len(doc.page_content) > 150 else doc.page_content
                print(f"   {i + 1}. –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page} | {source}")
                print(f"      üìÑ {preview}")
                print()
            else:
                print("\n‚ö†Ô∏è  –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

            return {
                "answer": result['result'],
                "source_documents": result['source_documents'],
                "question": question
            }

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞: {e}")
            return {
                "answer": "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.",
                "source_documents": [],
                "question": question,
                "error": str(e)
            }

    def search_similar(self, query: str, k: int = 3) -> List[Document]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LLM"""
        if not self.vectorstore:
            raise ValueError("–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ!")

        return self.vectorstore.similarity_search(query, k=k)

    def get_document_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ"""
        if not self.vectorstore:
            return {"status": "–î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã"}

        try:
            # –î–ª—è Chroma
            if hasattr(self.vectorstore, '_collection'):
                collection = self.vectorstore._collection
                count = collection.count() if collection else 0
            # –î–ª—è FAISS - –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç
            elif hasattr(self.vectorstore, 'index'):
                count = self.vectorstore.index.ntotal if hasattr(self.vectorstore.index, 'ntotal') else "Unknown"
            else:
                count = "Unknown"

            return {
                "document_path": self.pdf_path,
                "vector_store": self.persist_directory,
                "document_count": count,
                "embedding_model": "all-MiniLM-L6-v2",
                "status": "–ó–∞–≥—Ä—É–∂–µ–Ω–æ"
            }
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: {e}")
            return {
                "document_path": self.pdf_path,
                "status": "–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
            }

    def clear_vectorstore(self):
        """–û—á–∏—Å—Ç–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        if os.path.exists(self.persist_directory):
            shutil.rmtree(self.persist_directory)
            self.logger.info("–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –æ—á–∏—â–µ–Ω–æ")
        self.vectorstore = None
        self.qa_chain = None
